

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>FAQ &mdash; parallelformers 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Contributing to Parallelformers" href="CONTRIBUTING.html" />
    <link rel="prev" title="Policy Class" href="POLICY.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> parallelformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="INTRO.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="SUPPORTED_MODELS.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="POLICY.html">Policy Class</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#q-why-doesn-t-the-gpu-usage-decrease-by-exactly-n-times-when-i-parallelize-on-n-gpus">Q. Why doesn’t the GPU usage decrease by exactly <em>n</em> times when I parallelize on <em>n</em> GPUs?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#q-how-many-gpus-are-good-to-use-when-parallelizing-a-model">Q. How many GPUs are good to use when parallelizing a model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#q-why-are-some-models-not-supported">Q. Why are some models not supported?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#q-can-i-parallelize-multiple-models-on-same-gpus">Q. Can I parallelize multiple models on same GPUs?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#q-can-i-use-it-on-docker">Q. Can I use it on Docker?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributing to Parallelformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallelize</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelize/Parallelize.html">Parallelize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Engine.html">Parallel Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Process.html">Parallel Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Replacing.html">Tensor Replacer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Slicing.html">Tensor Slicer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Policy modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../policy/Auto.html">Auto Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../policy/Policy.html">Policy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/Attr_utils.html">Attribute Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/Dist_utils.html">Distributed Utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">parallelformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>FAQ</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/intro/FAQ.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h1>
<div class="section" id="q-why-doesn-t-the-gpu-usage-decrease-by-exactly-n-times-when-i-parallelize-on-n-gpus">
<h2>Q. Why doesn’t the GPU usage decrease by exactly <em>n</em> times when I parallelize on <em>n</em> GPUs?<a class="headerlink" href="#q-why-doesn-t-the-gpu-usage-decrease-by-exactly-n-times-when-i-parallelize-on-n-gpus" title="Permalink to this headline">¶</a></h2>
<p>There are three possible reasons.</p>
<ol class="simple">
<li><p>There are non-parallelizable areas in the model. For example, embedding, normalization and lm head layers can NOT be parallelized, resulting that they are copied to all GPUs.</p></li>
<li><p>We need to allocate shared memory areas for inter-process communication. Since this shared memory is allocated across all GPU processes, the GPU usage should increase.</p></li>
<li><p>When input tensors are copied to all GPUs, the GPU usage can increase.</p></li>
</ol>
</div>
<div class="section" id="q-how-many-gpus-are-good-to-use-when-parallelizing-a-model">
<h2>Q. How many GPUs are good to use when parallelizing a model?<a class="headerlink" href="#q-how-many-gpus-are-good-to-use-when-parallelizing-a-model" title="Permalink to this headline">¶</a></h2>
<p>We recommend you keep the number of GPUs as least as possible.</p>
</div>
<div class="section" id="q-why-are-some-models-not-supported">
<h2>Q. Why are some models not supported?<a class="headerlink" href="#q-why-are-some-models-not-supported" title="Permalink to this headline">¶</a></h2>
<p>There are several factors.
Models are partly supported or not supported if they …</p>
<ul class="simple">
<li><p>have dynamically changed layers.</p></li>
</ul>
<p>We only can parallelize static layers because the parallelization process should be completed before the forward pass. But some models’ layers (e.g., <code class="docutils literal notranslate"><span class="pre">BigBird's</span> <span class="pre">Self-Attention</span></code>) can change dynamically during the forward pass and ends up to unparallelization. For example, <code class="docutils literal notranslate"><span class="pre">BigBirdPegasus</span></code> contains <code class="docutils literal notranslate"><span class="pre">BigBird's</span> <span class="pre">Self-Attention</span></code> in its encoder layers, so they can’t be parallelized.</p>
<ul class="simple">
<li><p>have convolutional layers.</p></li>
</ul>
<p>The convolution operation is not compatible with the tensor slicing method. For example, the attention layers of <code class="docutils literal notranslate"><span class="pre">ConvBERT</span></code> and all the layers of <code class="docutils literal notranslate"><span class="pre">SqueezeBERT</span></code> consist of convolutions, so they can not be parallelized. It is worth mentioning that although OpenAI’s <code class="docutils literal notranslate"><span class="pre">GPT1</span></code> and <code class="docutils literal notranslate"><span class="pre">GPT2</span></code> also use convolutional operations, they can be parallelized because they actually perform matrix multiplication-based operations rather than actual convolutional operations. (Check the implementations of the <code class="docutils literal notranslate"><span class="pre">transformers.modeling_utils.Conv1D</span></code> layer)</p>
<ul class="simple">
<li><p>have n-gram attention layers.</p></li>
</ul>
<p>We conducted several parallelization experiments with <code class="docutils literal notranslate"><span class="pre">ProphetNet</span></code> that adopts the N-gram attention mechanism. Unfortunately, we found the results after the parallelization are not the same as the original representations for some reason.</p>
<ul class="simple">
<li><p>adopt <code class="docutils literal notranslate"><span class="pre">EncoderDecoderModel</span></code>.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">EncoderDecoderModel</span></code> framework conflicts with our <code class="docutils literal notranslate"><span class="pre">AutoPolicy</span></code> mechanism. Therefore, when using the <code class="docutils literal notranslate"><span class="pre">EncoderDecoderModel</span></code> framework, you have to write your own custom policy objects.</p>
<ul class="simple">
<li><p>can not be serialized.</p></li>
</ul>
<p>When transfering a model to other processes, the model’s weights must be serialized. Thus, the models that are not serializable such as <code class="docutils literal notranslate"><span class="pre">RAG</span></code> are do not support parallelization.</p>
</div>
<div class="section" id="q-can-i-parallelize-multiple-models-on-same-gpus">
<h2>Q. Can I parallelize multiple models on same GPUs?<a class="headerlink" href="#q-can-i-parallelize-multiple-models-on-same-gpus" title="Permalink to this headline">¶</a></h2>
<p>Yes. The following is example of multiple model parallelization. Note it is helpful to change the <code class="docutils literal notranslate"><span class="pre">master_port</span></code> if you want to parallelize multiple models on the same main process.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example of multiple model parallelization</span>

<span class="n">parallelize</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">master_port</span><span class="o">=</span><span class="mi">29500</span><span class="p">)</span>
<span class="n">parallelize</span><span class="p">(</span><span class="n">model_2</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">master_port</span><span class="o">=</span><span class="mi">29501</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="q-can-i-use-it-on-docker">
<h2>Q. Can I use it on Docker?<a class="headerlink" href="#q-can-i-use-it-on-docker" title="Permalink to this headline">¶</a></h2>
<p>Yes, but please note the followings.</p>
<ul class="simple">
<li><p>Issue about multiple model parallelization in Docker containers</p></li>
</ul>
<p>In Docker containsers, you can’t parallelize multiple models on the same GPUs. For example, If you parallelize first model to GPUs <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code>, you can not parallelize second model to GPUs <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code> again.</p>
<p>This is a bug between Docker and <code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> package in the Python. If you already have semophores on your main process and you are using the <code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> module with <code class="docutils literal notranslate"><span class="pre">spawn</span></code> method, Python try to check leaked semaphores (please check <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods">here</a>), and this leaked semaphore checker makes some problems in the Docker containers. (you can check more details <a class="reference external" href="https://stackoverflow.com/questions/54650904/python-multiprocessing-crashes-docker-container">here</a>, And we are currently working to solve this issue)</p>
<ul class="simple">
<li><p>Issue about shared memory size</p></li>
</ul>
<p>In addition, you need to increase the shared memory size if you want to use distributed backed such as <code class="docutils literal notranslate"><span class="pre">nccl</span></code> in Docker containers (The default is set to 2mb, but it is too small)</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="CONTRIBUTING.html" class="btn btn-neutral float-right" title="Contributing to Parallelformers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="POLICY.html" class="btn btn-neutral float-left" title="Policy Class" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, TUNiB Inc.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>