

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>&lt;no title&gt; &mdash; parallelformers 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="&lt;no title&gt;" href="SUPPORTED_MODELS.html" />
    <link rel="prev" title="Parallformers: Yet Another Versatile Transformer Model Paraelleization Toolkit" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> parallelformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Parallelize</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelize/Parallelize.html">Parallelize</a></li>
</ul>
<p class="caption"><span class="caption-text">Parallel modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Engine.html">Parallel Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Process.html">Parallel Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Replacing.html">Tensor Replacer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Slicing.html">Tensor Slicer</a></li>
</ul>
<p class="caption"><span class="caption-text">Policy modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../policy/Auto.html">Auto Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../policy/Policy.html">Policy</a></li>
</ul>
<p class="caption"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/Attr_utils.html">Attribute Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/Dist_utils.html">Distributed Utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">parallelformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/intro/INTRO.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p>## Why Parallelformers?
You can load a model that is too large for a single GPU. For example, using Parallelformers, you can load a model of 12GB on two 8 GB GPUs. In addition, you can save your precious money because usually multiple smaller size GPUs are less costly than a single larger size GPU.</p>
<p>## Installation
Parallelformers can be easily installed using the <cite>pip</cite> package manager. All the dependencies such as [torch](<a class="reference external" href="https://pypi.org/project/torch/">https://pypi.org/project/torch/</a>), [transformers](<a class="reference external" href="https://pypi.org/project/transformers/">https://pypi.org/project/transformers/</a>), and [dacite](<a class="reference external" href="https://pypi.org/project/dacite/">https://pypi.org/project/dacite/</a>) should be installed automatically with the following command. Be careful that the name is plural.
<code class="docutils literal notranslate"><span class="pre">`console</span>
<span class="pre">pip</span> <span class="pre">install</span> <span class="pre">parallelformers</span>
<span class="pre">`</span></code></p>
<p>## Getting Started
#### 1. Create a HuggingFace transformers model.
You don’t need to call <cite>.half()</cite> or <cite>.cuda()</cite> as those functions will be invoked automatically. It is more memory efficient to start parallelization on the CPU.
<a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>model = AutoModelForCausalLM.from_pretrained(“EleutherAI/gpt-neo-2.7B”)
tokenizer = AutoTokenizer.from_pretrained(“EleutherAI/gpt-neo-2.7B”)
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>#### 2. Put the <cite>model</cite> in the <cite>parallelize()</cite> function.
<a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
from parallelformers import parallelize</p>
<p>parallelize(model, num_gpus=2, fp16=True, verbose=’detail’)
<a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p>Since <cite>nvidia-smi</cite> shows the reserved cache area, it is difficult to check the exact allocated memory. To check the allocated memory state well, <strong>you can set the verbose option as `’detail’` or `’simple’`.</strong> (default is <cite>None</cite>)</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a>`
<a href="#id55"><span class="problematic" id="id56">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 0                 |
<a href="#id57"><span class="problematic" id="id58">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id59"><span class="problematic" id="id60">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id61"><span class="problematic" id="id62">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |    2721 MB |    2967 MB |    2967 MB |  251905 KB |
|       from large pool |    2720 MB |    2966 MB |    2966 MB |  251904 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |       1 KB |
<a href="#id63"><span class="problematic" id="id64">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU:0 =&gt; 2.72GB
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
<a href="#id65"><span class="problematic" id="id66">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 1                 |
<a href="#id67"><span class="problematic" id="id68">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id69"><span class="problematic" id="id70">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id71"><span class="problematic" id="id72">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |    2721 MB |    2967 MB |    2967 MB |  251905 KB |
|       from large pool |    2720 MB |    2966 MB |    2966 MB |  251904 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |       1 KB |
<a href="#id73"><span class="problematic" id="id74">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU:1 =&gt; 2.72GB
<a href="#id19"><span class="problematic" id="id20">``</span></a><a href="#id21"><span class="problematic" id="id22">`</span></a></p>
<p>#### 3. Do Inference as usual.
You don’t have to call <cite>.cuda()</cite> when creating input tokens. <strong>Note that you should input both input tokens and attention masks to the model.</strong> (<cite>**inputs</cite> is the recommended way for this)
<a href="#id23"><span class="problematic" id="id24">``</span></a><a href="#id25"><span class="problematic" id="id26">`</span></a>python
inputs = tokenizer(“Parallelformers is”, return_tensors=”pt”)</p>
<dl class="simple">
<dt>outputs = model.generate(</dt><dd><p><a href="#id27"><span class="problematic" id="id28">**</span></a>inputs,
num_beams=5,
no_repeat_ngram_size=4,
max_length=15,</p>
</dd>
</dl>
<p>)</p>
<p>print(f”Output: {tokenizer.batch_decode(outputs)[0]}”)
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
Output: Parallelformers is an open-source library for parallel programming …
<a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
<p>#### 4. Deploy the model to the server as usual.
The parallelization process does not affect the web server because they are automatically synchronized.
<a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>python
from flask import Flask</p>
<p>app = Flask(__name__)</p>
<p>&#64;app.route(“/generate_text/&lt;text&gt;”)
def generate_text(text):</p>
<blockquote>
<div><p>inputs = tokenizer(text, return_tensors=”pt”)</p>
<dl class="simple">
<dt>outputs = model.generate(</dt><dd><p><a href="#id37"><span class="problematic" id="id38">**</span></a>inputs,
num_beams=5,
no_repeat_ngram_size=4,
max_length=15,</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>outputs = tokenizer.batch_decode(</dt><dd><p>outputs,
skip_special_tokens=True,</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>return {</dt><dd><p>“inputs”: text,
“outputs”: outputs[0],</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>app.run(host=”0.0.0.0”, port=5000)
<a href="#id39"><span class="problematic" id="id40">``</span></a><a href="#id41"><span class="problematic" id="id42">`</span></a></p>
<p>You can send a request to the web server as follows:
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">$</span> <span class="pre">curl</span> <span class="pre">-X</span> <span class="pre">get</span> <span class="pre">&quot;YOUR_IP:5000/generate_text/Messi&quot;</span>
<span class="pre">`</span></code>
And the following result should be returned.
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">{&quot;inputs&quot;:</span> <span class="pre">&quot;Messi&quot;,</span> <span class="pre">&quot;outputs&quot;:</span> <span class="pre">&quot;Messi</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">best</span> <span class="pre">player</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">world</span> <span class="pre">right</span> <span class="pre">now.</span> <span class="pre">He</span> <span class="pre">is</span> <span class="pre">the&quot;}</span>
<span class="pre">`</span></code></p>
<p>#### 5. Check the current GPU states.
You can check GPU states using <cite>.memory_allocated()</cite>, <cite>.memory_reserved()</cite> and <cite>.memory_chached()</cite> to make sure the parallelization is successful.
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">model.memory_allocated()</span>
<span class="pre">model.memory_reserved()</span>
<span class="pre">model.memory_chached()</span>
<span class="pre">`</span></code>
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">{'cuda:0':XXXXXX,</span> <span class="pre">'cuda:1':XXXXXX}</span>
<span class="pre">`</span></code></p>
<p>#### 6. Manage the model parallelization states.
You can manage model parallelization states using <cite>.cuda()</cite>, <cite>.cpu()</cite> and <cite>.to()</cite>. <strong>The model parallelization process ends if you call those functions.</strong>
<a href="#id43"><span class="problematic" id="id44">``</span></a><a href="#id45"><span class="problematic" id="id46">`</span></a>python
model.cuda()</p>
<p>print(torch.cuda.memory_summary(0))
print(torch.cuda.memory_summary(1))
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">Check</span> <span class="pre">the</span> <span class="pre">allocated</span> <span class="pre">memory</span> <span class="pre">status</span> <span class="pre">using</span> <span class="pre">`torch.cuda.memory_summary()`.</span>
<span class="pre">`</span></code>
<a href="#id75"><span class="problematic" id="id76">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 0                 |
<a href="#id77"><span class="problematic" id="id78">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id79"><span class="problematic" id="id80">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id81"><span class="problematic" id="id82">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |    5121 MB |    5121 MB |    5121 MB |    1024 B  |
|       from large pool |    5120 MB |    5120 MB |    5120 MB |       0 B  |
|       from small pool |       1 MB |       1 MB |       1 MB |    1024 B  |
<a href="#id83"><span class="problematic" id="id84">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU0 =&gt; 5.12GB
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
<a href="#id85"><span class="problematic" id="id86">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 1                 |
<a href="#id87"><span class="problematic" id="id88">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id89"><span class="problematic" id="id90">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id91"><span class="problematic" id="id92">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |       0 B  |    1024 B  |    1024 B  |    1024 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |    1024 B  |    1024 B  |    1024 B  |
<a href="#id93"><span class="problematic" id="id94">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU1 =&gt; 0.00GB
<a href="#id47"><span class="problematic" id="id48">``</span></a>`
If you switch to the CPU mode, it works like this.
<a href="#id49"><span class="problematic" id="id50">``</span></a><a href="#id51"><span class="problematic" id="id52">`</span></a>python
model.cpu()</p>
<p>print(torch.cuda.memory_summary(0))
print(torch.cuda.memory_summary(1))
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
<a href="#id95"><span class="problematic" id="id96">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 0                 |
<a href="#id97"><span class="problematic" id="id98">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id99"><span class="problematic" id="id100">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id101"><span class="problematic" id="id102">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |       0 B  |    5121 MB |    5121 MB |    5121 MB |
|       from large pool |       0 B  |    5120 MB |    5120 MB |    5120 MB |
|       from small pool |       0 B  |       1 MB |       1 MB |       1 MB |
<a href="#id103"><span class="problematic" id="id104">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU0 =&gt; 0.00GB
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
<a href="#id105"><span class="problematic" id="id106">|===========================================================================|</span></a>
|                  PyTorch CUDA memory summary, device ID 1                 |
<a href="#id107"><span class="problematic" id="id108">|---------------------------------------------------------------------------|</span></a>
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
<a href="#id109"><span class="problematic" id="id110">|===========================================================================|</span></a>
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
<a href="#id111"><span class="problematic" id="id112">|---------------------------------------------------------------------------|</span></a>
| Allocated memory      |       0 B  |    1024 B  |    1024 B  |    1024 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |    1024 B  |    1024 B  |    1024 B  |
<a href="#id113"><span class="problematic" id="id114">|---------------------------------------------------------------------------|</span></a></p>
<p>GPU1 =&gt; 0.00GB
<a href="#id53"><span class="problematic" id="id54">``</span></a>`
#### 7. Write <cite>Policy</cite> class when new models are released.
Refer to [POLICY.md](POLICY.md)</p>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="SUPPORTED_MODELS.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../index.html" class="btn btn-neutral float-left" title="Parallformers: Yet Another Versatile Transformer Model Paraelleization Toolkit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, TUNiB Inc.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>