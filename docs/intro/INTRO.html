

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Introduction &mdash; parallelformers 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Supported Models" href="SUPPORTED_MODELS.html" />
    <link rel="prev" title="Parallformers: An Efficient Model Parallelization Toolkit for Deployment" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> parallelformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#why-parallelformers">Why Parallelformers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-a-huggingface-transformers-model">1. Create a HuggingFace transformers model.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#put-the-model-in-the-parallelize-function">2. Put the <code class="docutils literal notranslate"><span class="pre">model</span></code> in the <code class="docutils literal notranslate"><span class="pre">parallelize()</span></code> function.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#do-inference-as-usual">3. Do Inference as usual.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-the-model-to-the-server-as-usual">4. Deploy the model to the server as usual.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#check-the-current-gpu-states">5. Check the current GPU states.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#manage-the-model-parallelization-states">6. Manage the model parallelization states.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#write-policy-class-when-new-models-are-released">7. Write <code class="docutils literal notranslate"><span class="pre">Policy</span></code> class when new models are released.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="SUPPORTED_MODELS.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="POLICY.html">Policy Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributing to Parallelformers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallelize</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelize/Parallelize.html">Parallelize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Engine.html">Parallel Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Process.html">Parallel Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Replacing.html">Tensor Replacer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallel/Slicing.html">Tensor Slicer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Policy modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../policy/Auto.html">Auto Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../policy/Policy.html">Policy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/Attr_utils.html">Attribute Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/Dist_utils.html">Distributed Utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">parallelformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/intro/INTRO.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<div class="section" id="why-parallelformers">
<h2>Why Parallelformers?<a class="headerlink" href="#why-parallelformers" title="Permalink to this headline">¶</a></h2>
<p>You can load a model that is too large for a single GPU. For example, using Parallelformers, you can load a model of 12GB on two 8 GB GPUs. In addition, you can save your precious money because usually multiple smaller size GPUs are less costly than a single larger size GPU.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Parallelformers can be easily installed using the <code class="docutils literal notranslate"><span class="pre">pip</span></code> package manager. All the dependencies such as <a class="reference external" href="https://pypi.org/project/torch/">torch</a>, <a class="reference external" href="https://pypi.org/project/transformers/">transformers</a>, and <a class="reference external" href="https://pypi.org/project/dacite/">dacite</a> should be installed automatically with the following command. Be careful that the name is plural.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pip install parallelformers</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<div class="section" id="create-a-huggingface-transformers-model">
<h3>1. Create a HuggingFace transformers model.<a class="headerlink" href="#create-a-huggingface-transformers-model" title="Permalink to this headline">¶</a></h3>
<p>You don’t need to call <code class="docutils literal notranslate"><span class="pre">.half()</span></code> or <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> as those functions will be invoked automatically. It is more memory efficient to start parallelization on the CPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;EleutherAI/gpt-neo-2.7B&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;EleutherAI/gpt-neo-2.7B&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="put-the-model-in-the-parallelize-function">
<h3>2. Put the <code class="docutils literal notranslate"><span class="pre">model</span></code> in the <code class="docutils literal notranslate"><span class="pre">parallelize()</span></code> function.<a class="headerlink" href="#put-the-model-in-the-parallelize-function" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">parallelformers</span> <span class="kn">import</span> <span class="n">parallelize</span>

<span class="n">parallelize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="s1">&#39;detail&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Since <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> shows the reserved cache area, it is difficult to check the exact allocated memory. To check the allocated memory state well, <strong>you can set the verbose option as <code class="docutils literal notranslate"><span class="pre">'detail'</span></code> or <code class="docutils literal notranslate"><span class="pre">'simple'</span></code>.</strong> (default is <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">0</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>    <span class="mi">2721</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2967</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2967</span> <span class="n">MB</span> <span class="o">|</span>  <span class="mi">251905</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>    <span class="mi">2720</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2966</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2966</span> <span class="n">MB</span> <span class="o">|</span>  <span class="mi">251904</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU</span><span class="p">:</span><span class="mi">0</span> <span class="o">=&gt;</span> <span class="mf">2.72</span><span class="n">GB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">1</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>    <span class="mi">2721</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2967</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2967</span> <span class="n">MB</span> <span class="o">|</span>  <span class="mi">251905</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>    <span class="mi">2720</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2966</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">2966</span> <span class="n">MB</span> <span class="o">|</span>  <span class="mi">251904</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">KB</span> <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU</span><span class="p">:</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="mf">2.72</span><span class="n">GB</span>
</pre></div>
</div>
</div>
<div class="section" id="do-inference-as-usual">
<h3>3. Do Inference as usual.<a class="headerlink" href="#do-inference-as-usual" title="Permalink to this headline">¶</a></h3>
<p>You don’t have to call <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> when creating input tokens. <strong>Note that you should input both input tokens and attention masks to the model.</strong> (<code class="docutils literal notranslate"><span class="pre">**inputs</span></code> is the recommended way for this)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Parallelformers is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Output</span><span class="p">:</span> <span class="n">Parallelformers</span> <span class="ow">is</span> <span class="n">an</span> <span class="nb">open</span><span class="o">-</span><span class="n">source</span> <span class="n">library</span> <span class="k">for</span> <span class="n">parallel</span> <span class="n">programming</span> <span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="deploy-the-model-to-the-server-as-usual">
<h3>4. Deploy the model to the server as usual.<a class="headerlink" href="#deploy-the-model-to-the-server-as-usual" title="Permalink to this headline">¶</a></h3>
<p>The parallelization process does not affect the web server because they are automatically synchronized.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Flask</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s2">&quot;/generate_text/&lt;text&gt;&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span>
        <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span>
        <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">}</span>


<span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>You can send a request to the web server as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ curl -X get &quot;YOUR_IP:5000/generate_text/Messi&quot;
</pre></div>
</div>
<p>And the following result should be returned.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="s2">&quot;Messi&quot;</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="s2">&quot;Messi is the best player in the world right now. He is the&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="check-the-current-gpu-states">
<h3>5. Check the current GPU states.<a class="headerlink" href="#check-the-current-gpu-states" title="Permalink to this headline">¶</a></h3>
<p>You can check GPU states using <code class="docutils literal notranslate"><span class="pre">.memory_allocated()</span></code>, <code class="docutils literal notranslate"><span class="pre">.memory_reserved()</span></code> and <code class="docutils literal notranslate"><span class="pre">.memory_chached()</span></code> to make sure the parallelization is successful.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">memory_chached</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">:</span><span class="n">XXXXXX</span><span class="p">,</span> <span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="n">XXXXXX</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="manage-the-model-parallelization-states">
<h3>6. Manage the model parallelization states.<a class="headerlink" href="#manage-the-model-parallelization-states" title="Permalink to this headline">¶</a></h3>
<p>You can manage model parallelization states using <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code>, <code class="docutils literal notranslate"><span class="pre">.cpu()</span></code> and <code class="docutils literal notranslate"><span class="pre">.to()</span></code>. <strong>The model parallelization process ends if you call those functions.</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>Check the allocated memory status using <code class="docutils literal notranslate"><span class="pre">torch.cuda.memory_summary()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">0</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU0</span> <span class="o">=&gt;</span> <span class="mf">5.12</span><span class="n">GB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">1</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU1</span> <span class="o">=&gt;</span> <span class="mf">0.00</span><span class="n">GB</span>
</pre></div>
</div>
<p>If you switch to the CPU mode, it works like this.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">0</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5121</span> <span class="n">MB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>    <span class="mi">5120</span> <span class="n">MB</span> <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>       <span class="mi">1</span> <span class="n">MB</span> <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU0</span> <span class="o">=&gt;</span> <span class="mf">0.00</span><span class="n">GB</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|===========================================================================|</span>
<span class="o">|</span>                  <span class="n">PyTorch</span> <span class="n">CUDA</span> <span class="n">memory</span> <span class="n">summary</span><span class="p">,</span> <span class="n">device</span> <span class="n">ID</span> <span class="mi">1</span>                 <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span>            <span class="n">CUDA</span> <span class="n">OOMs</span><span class="p">:</span> <span class="mi">0</span>            <span class="o">|</span>        <span class="n">cudaMalloc</span> <span class="n">retries</span><span class="p">:</span> <span class="mi">0</span>         <span class="o">|</span>
<span class="o">|===========================================================================|</span>
<span class="o">|</span>        <span class="n">Metric</span>         <span class="o">|</span> <span class="n">Cur</span> <span class="n">Usage</span>  <span class="o">|</span> <span class="n">Peak</span> <span class="n">Usage</span> <span class="o">|</span> <span class="n">Tot</span> <span class="n">Alloc</span>  <span class="o">|</span> <span class="n">Tot</span> <span class="n">Freed</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>
<span class="o">|</span> <span class="n">Allocated</span> <span class="n">memory</span>      <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">large</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|</span>       <span class="kn">from</span> <span class="nn">small</span> <span class="n">pool</span> <span class="o">|</span>       <span class="mi">0</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>    <span class="mi">1024</span> <span class="n">B</span>  <span class="o">|</span>
<span class="o">|---------------------------------------------------------------------------|</span>

<span class="n">GPU1</span> <span class="o">=&gt;</span> <span class="mf">0.00</span><span class="n">GB</span>
</pre></div>
</div>
</div>
<div class="section" id="write-policy-class-when-new-models-are-released">
<h3>7. Write <code class="docutils literal notranslate"><span class="pre">Policy</span></code> class when new models are released.<a class="headerlink" href="#write-policy-class-when-new-models-are-released" title="Permalink to this headline">¶</a></h3>
<p>Refer to <a class="reference internal" href="POLICY.html"><span class="doc">POLICY.md</span></a></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="SUPPORTED_MODELS.html" class="btn btn-neutral float-right" title="Supported Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../index.html" class="btn btn-neutral float-left" title="Parallformers: An Efficient Model Parallelization Toolkit for Deployment" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, TUNiB Inc.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>